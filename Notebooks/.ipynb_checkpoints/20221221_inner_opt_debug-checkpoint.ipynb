{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6017e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.dynamics_models' from '../models/dynamics_models.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "from models import dynamics_models as dm\n",
    "from importlib import reload\n",
    "import torch.autograd as autograd\n",
    "\n",
    "reload(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86349566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "inner_args = {'normalize': 0, 'batchsize': 32, 'num_loops': 20, 'detach': 1, 'beta': 0, 'temperature': 1, 'mode': 'exact', 'lr': 0.01, 'dim_a': 16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8d3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H0H1.pkl', 'rb') as fp:\n",
    "    problem = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4872338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = problem['H0']\n",
    "H1 = problem['H1']\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "trueM = np.random.normal(size = (H0.shape[0], 16, 16))\n",
    "trueH1 = H0 @ trueM\n",
    "inner_args['num_loops']=200\n",
    "inner_args['lr']=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3708779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../models/dynamics_models.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.Meta_M = Variable(torch.tensor(einops.repeat(torch.eye(inner_args['dim_a']), 'c a -> b c a', b = inner_args['batchsize'])), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dy_obj = dm.LinearTensorDynamicsLSTSQ_inner(inner_args=inner_args, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d8de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "inner_loss 2801.8849043935033\n",
      "inner_loss 2801.114975768376\n",
      "inner_loss 2800.3763066169354\n",
      "inner_loss 2799.6686305187823\n",
      "inner_loss 2798.991681904543\n",
      "inner_loss 2798.3451940420855\n",
      "inner_loss 2797.728898938072\n",
      "inner_loss 2797.142529026388\n",
      "inner_loss 2796.585816559075\n",
      "inner_loss 2796.058495568236\n",
      "inner_loss 2795.5602969339056\n",
      "inner_loss 2795.0909542414392\n",
      "inner_loss 2794.6501972520505\n",
      "inner_loss 2794.23775888056\n",
      "inner_loss 2793.8533710824\n",
      "inner_loss 2793.4967681851763\n",
      "inner_loss 2793.167679194748\n",
      "inner_loss 2792.865840497102\n",
      "inner_loss 2792.5909823531847\n",
      "inner_loss 2792.342839359107\n",
      "inner_loss 2792.121146048926\n",
      "inner_loss 2791.925633726778\n",
      "inner_loss 2791.7560409492235\n",
      "inner_loss 2791.6120986049827\n",
      "inner_loss 2791.493548403734\n",
      "inner_loss 2791.400122128678\n",
      "inner_loss 2791.331558752104\n",
      "inner_loss 2791.2875968916856\n",
      "inner_loss 2791.2679730773434\n",
      "inner_loss 2791.2724278197843\n",
      "inner_loss 2791.3007010302954\n",
      "inner_loss 2791.352533605271\n",
      "inner_loss 2791.4276677855974\n",
      "inner_loss 2791.525846594261\n",
      "inner_loss 2791.6468131017464\n",
      "inner_loss 2791.7903132810106\n",
      "inner_loss 2791.9560903832166\n",
      "inner_loss 2792.1438929310616\n",
      "inner_loss 2792.353469585043\n",
      "inner_loss 2792.5845684122582\n",
      "inner_loss 2792.8369395081795\n",
      "inner_loss 2793.1103346468503\n",
      "inner_loss 2793.4045055266165\n",
      "inner_loss 2793.719206769687\n",
      "inner_loss 2794.054191710475\n",
      "inner_loss 2794.409220257309\n",
      "inner_loss 2794.7840471288814\n",
      "inner_loss 2795.1784310796857\n",
      "inner_loss 2795.592133816509\n",
      "inner_loss 2796.024916739831\n",
      "inner_loss 2796.4765423200884\n",
      "inner_loss 2796.9467734457694\n",
      "inner_loss 2797.435379736343\n",
      "inner_loss 2797.942127651593\n",
      "inner_loss 2798.4667835730875\n",
      "inner_loss 2799.0091187992334\n",
      "inner_loss 2799.5689053313604\n",
      "inner_loss 2800.1459156148403\n",
      "inner_loss 2800.739924460363\n",
      "inner_loss 2801.350709455243\n",
      "inner_loss 2801.9780448538877\n",
      "inner_loss 2802.6217150851035\n",
      "inner_loss 2803.2815001565386\n",
      "inner_loss 2803.957181087758\n",
      "inner_loss 2804.6485405621956\n",
      "inner_loss 2805.3553651790244\n",
      "inner_loss 2806.0774426520575\n",
      "inner_loss 2806.8145630903477\n",
      "inner_loss 2807.566515305467\n",
      "inner_loss 2808.333093369286\n",
      "inner_loss 2809.1140898833237\n",
      "inner_loss 2809.9093001099745\n",
      "inner_loss 2810.718519091754\n",
      "inner_loss 2811.541550297085\n",
      "inner_loss 2812.378191094648\n",
      "inner_loss 2813.2282440090185\n",
      "inner_loss 2814.0915147475207\n",
      "inner_loss 2814.9678039473447\n",
      "inner_loss 2815.856922854464\n",
      "inner_loss 2816.7586782971152\n",
      "inner_loss 2817.672882043074\n",
      "inner_loss 2818.5993469401596\n",
      "inner_loss 2819.537886085355\n",
      "inner_loss 2820.488312666521\n",
      "inner_loss 2821.4504454977773\n",
      "inner_loss 2822.4241035667446\n",
      "inner_loss 2823.409105713087\n",
      "inner_loss 2824.4052751807517\n",
      "inner_loss 2825.4124375903443\n",
      "inner_loss 2826.430416004888\n",
      "inner_loss 2827.4590391339943\n",
      "inner_loss 2828.498135474582\n",
      "inner_loss 2829.5475340799344\n",
      "inner_loss 2830.6070660406967\n",
      "inner_loss 2831.676567072927\n",
      "inner_loss 2832.7558729067005\n",
      "inner_loss 2833.8448183816513\n",
      "inner_loss 2834.943241421948\n",
      "inner_loss 2836.050983072673\n",
      "inner_loss 2837.1678869908296\n",
      "inner_loss 2838.2937981556374\n",
      "inner_loss 2839.428556602855\n",
      "inner_loss 2840.572011292688\n",
      "inner_loss 2841.724011343938\n",
      "inner_loss 2842.8844054773667\n",
      "inner_loss 2844.053045044704\n",
      "inner_loss 2845.2297829228723\n",
      "inner_loss 2846.4144744131436\n",
      "inner_loss 2847.606974683814\n",
      "inner_loss 2848.8071413584494\n",
      "inner_loss 2850.014832767644\n",
      "inner_loss 2851.229909112639\n",
      "inner_loss 2852.4522353409043\n",
      "inner_loss 2853.681672983844\n",
      "inner_loss 2854.918085974588\n",
      "inner_loss 2856.161344128385\n",
      "inner_loss 2857.4113116347726\n",
      "inner_loss 2858.667860463657\n",
      "inner_loss 2859.930861615736\n",
      "inner_loss 2861.200186208798\n",
      "inner_loss 2862.4757103326465\n",
      "inner_loss 2863.757307286259\n",
      "inner_loss 2865.044853601872\n",
      "inner_loss 2866.3382280262886\n",
      "inner_loss 2867.637311105528\n",
      "inner_loss 2868.9419830905945\n",
      "inner_loss 2870.2521267565867\n",
      "inner_loss 2871.567625476944\n",
      "inner_loss 2872.888363257271\n",
      "inner_loss 2874.214225871316\n",
      "inner_loss 2875.5451037251755\n",
      "inner_loss 2876.880883618754\n",
      "inner_loss 2878.2214615180624\n",
      "inner_loss 2879.566719945924\n",
      "inner_loss 2880.916558969862\n",
      "inner_loss 2882.270868484011\n",
      "inner_loss 2883.6295478063284\n",
      "inner_loss 2884.9924904281334\n",
      "inner_loss 2886.359595354945\n",
      "inner_loss 2887.7307608656974\n",
      "inner_loss 2889.1058911617874\n",
      "inner_loss 2890.48488517884\n",
      "inner_loss 2891.8676463358133\n",
      "inner_loss 2893.2540768745853\n",
      "inner_loss 2894.644084987109\n",
      "inner_loss 2896.0375797317993\n",
      "inner_loss 2897.434464486097\n",
      "inner_loss 2898.834648526677\n",
      "inner_loss 2900.2380431548854\n",
      "inner_loss 2901.644555776887\n",
      "inner_loss 2903.0541025012258\n",
      "inner_loss 2904.4665951445504\n",
      "inner_loss 2905.881950005349\n",
      "inner_loss 2907.3000810810836\n",
      "inner_loss 2908.7209046993266\n",
      "inner_loss 2910.1443399995233\n",
      "inner_loss 2911.5703046525573\n",
      "inner_loss 2912.9987201418126\n",
      "inner_loss 2914.4295050760766\n",
      "inner_loss 2915.862581369774\n",
      "inner_loss 2917.2978732386964\n",
      "inner_loss 2918.7353036753784\n",
      "inner_loss 2920.1747977174014\n",
      "inner_loss 2921.616281288485\n",
      "inner_loss 2923.0596817516357\n",
      "inner_loss 2924.5049273668537\n",
      "inner_loss 2925.951944686567\n",
      "inner_loss 2927.4006659002966\n",
      "inner_loss 2928.8510202281345\n",
      "inner_loss 2930.302939181193\n",
      "inner_loss 2931.7563564726565\n",
      "inner_loss 2933.2112050393266\n",
      "inner_loss 2934.6674184073286\n",
      "inner_loss 2936.124931567779\n",
      "inner_loss 2937.5836820191103\n",
      "inner_loss 2939.043608733508\n",
      "inner_loss 2940.504646782734\n",
      "inner_loss 2941.9667330476436\n",
      "inner_loss 2943.42981360474\n",
      "inner_loss 2944.893822821746\n",
      "inner_loss 2946.358703291922\n",
      "inner_loss 2947.8243978824016\n",
      "inner_loss 2949.290848619326\n",
      "inner_loss 2950.75799952399\n",
      "inner_loss 2952.2257946409695\n",
      "inner_loss 2953.6941788035033\n",
      "inner_loss 2955.163098157233\n",
      "inner_loss 2956.6325010943974\n",
      "inner_loss 2958.102332867045\n",
      "inner_loss 2959.5725395481104\n",
      "inner_loss 2961.043076937124\n",
      "inner_loss 2962.51388762819\n",
      "inner_loss 2963.984928551378\n",
      "inner_loss 2965.4561446513985\n",
      "inner_loss 2966.9274919944155\n",
      "inner_loss 2968.398923177936\n",
      "inner_loss 2969.8703866867304\n",
      "inner_loss 2971.3418405971984\n",
      "inner_loss 2972.81323938732\n",
      "inner_loss 2974.28453548184\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "Mstar = dy_obj.compute_M(H0, trueH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56297afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.6199, -0.5921, -0.5196, -1.0728],\n",
       "         [-0.1867, -0.8487,  0.0347,  0.5715],\n",
       "         [-0.6773, -0.8347, -0.6568, -0.0241],\n",
       "         [ 0.1075,  0.6043,  0.2814, -0.3443]], grad_fn=<SliceBackward>),\n",
       " array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862],\n",
       "        [-0.17242821, -0.87785842,  0.04221375,  0.58281521],\n",
       "        [-0.6871727 , -0.84520564, -0.67124613, -0.0126646 ],\n",
       "        [ 0.12015895,  0.61720311,  0.30017032, -0.35224985]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mstar[0][:4, :4], trueM[0][:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83f082ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3686.7030, dtype=torch.float64, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.sum((H0@Mstar[:H0.shape[0]]- trueH1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d76c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278eece1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8ba4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338a555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fdc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11f64f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7896/523019437.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Meta_M = Variable(torch.tensor(torch.eye(inner_args['dim_a'])), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "Meta_M = Variable(torch.tensor(torch.eye(inner_args['dim_a'])), requires_grad=True)\n",
    "trueM_two = np.random.normal(size = (16, 16))\n",
    "\n",
    "H0p = torch.tensor(np.random.normal(size=(H0.shape))).float()\n",
    "H1hat = H0p @ trueM_two\n",
    "\n",
    "inner_args['lr']=0.01\n",
    "inner_args['num_loops']=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4062b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_loss 2633.1461279419805\n",
      "inner_loss 2727.464337781186\n",
      "inner_loss 2822.441931595981\n",
      "inner_loss 2917.9359411032265\n",
      "inner_loss 3013.8270243257793\n",
      "inner_loss 3110.0155785322463\n",
      "inner_loss 3206.4185339196474\n",
      "inner_loss 3302.9666561718127\n",
      "inner_loss 3399.6021011930384\n",
      "inner_loss 3496.276840967678\n",
      "inner_loss 3592.9506660496904\n",
      "inner_loss 3689.590409650774\n",
      "inner_loss 3786.1682993844415\n",
      "inner_loss 3882.661418677693\n",
      "inner_loss 3979.050815425295\n",
      "inner_loss 4075.3210978750512\n",
      "inner_loss 4171.459362778423\n",
      "inner_loss 4267.455387460015\n",
      "inner_loss 4363.300669640847\n",
      "inner_loss 4458.988804335975\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "with torch.enable_grad():\n",
    "\n",
    "    for j in range(inner_args['num_loops']):\n",
    "        loss = torch.sqrt(torch.sum((H0p@Meta_M - H1hat)**2))\n",
    "        print(f\"\"\"inner_loss {loss.item()}\"\"\")\n",
    "        grads = autograd.grad(loss, Meta_M,\n",
    "                              create_graph=(not inner_args['detach']),\n",
    "                              only_inputs=True, allow_unused=True)\n",
    "        # parameter update\n",
    "        # for param, grad in zip(self.mobject.parameters(), grads):\n",
    "        #\n",
    "        #     new_param = param - self.inner_args['lr'] * grad\n",
    "        #     #new_param = param - 0.001 * grad\n",
    "        #\n",
    "        #     param.data.copy_(new_param)\n",
    "        Meta_M = Meta_M - inner_args['lr'] * grads[0][0]\n",
    "print(\"-\"*10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d47b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
