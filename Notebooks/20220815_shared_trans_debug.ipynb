{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd97c387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20220617_default_run_mnist_bg',\n",
       " '20220624_Mstar_longer_tp_neuralM_comm',\n",
       " '20220621_NeuralMstar_bg_neuralM_vanilla',\n",
       " '20220624_Mstar_longer_tp_neuralM_vanilla',\n",
       " '20220630_NeuralMstar_comm_0',\n",
       " '20220617_NeuralMstar_bg_neuralM_comm',\n",
       " '20220628_NeuralMstar_comm_0',\n",
       " '20220621_NeuralMstar_bg_neuralM_comm',\n",
       " '20220621_so3_rcst_so3run',\n",
       " '20220615_default_run_mnist',\n",
       " '20220615_NeuralMstar',\n",
       " '20220615_NeuralMstar_neuralM_vanilla',\n",
       " '20220628_pfkube_try_0',\n",
       " '20220701_so3_various_dat_three_0',\n",
       " '20220630_longer_tp_0',\n",
       " '20220628_so3_idnet_0',\n",
       " '20220611_so3_so3run_rcst',\n",
       " '20220628_so3_various_dat_0',\n",
       " '20220701_so3_various_dat_two_0',\n",
       " '20220611_so3_so3run_rcst_iter',\n",
       " '20220701_so3_various_dat_four_0',\n",
       " '20220624_NeuralMstar_comm_bg_neuralM_vanilla',\n",
       " '20220628_so3_linear_net_0',\n",
       " '20220624_NeuralMstar_comm_bg_neuralM_comm',\n",
       " '20220630_so3_various_dat_0',\n",
       " '20220511_so3_exp_0',\n",
       " 'temp',\n",
       " '20220629_so3_various_dat_0',\n",
       " '20220628_longer_tp_0',\n",
       " '20220611_so3_so3run',\n",
       " '20220611_so3',\n",
       " '20220621_default_run_mnist_bg',\n",
       " '20220615_Mstar_comm',\n",
       " '20220617_NeuralMstar_bg_neuralM_vanilla',\n",
       " '20220615_NeuralMstar_neuralM',\n",
       " '20220621_so3_rcst_so3run_rcst']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../datasets')\n",
    "sys.path.append('../models')\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload\n",
    "from datasets.three_dim_shapes import ThreeDimShapesDataset\n",
    "from datasets.small_norb import SmallNORBDataset\n",
    "from datasets.seq_mnist import SequentialMNIST\n",
    "import models.seqae as seqae\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from datasets import so3_data as so3d\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import r2_score\n",
    "import pdb\n",
    "from einops import rearrange\n",
    "\n",
    "import copy\n",
    "\n",
    "import csv\n",
    "import ast\n",
    "from source import yaml_utils as yu\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    gpu_index = -1\n",
    "\n",
    "    \n",
    "    \n",
    "from utils import yaml_utils as yu\n",
    "import yaml\n",
    "rootpath = '/mnt/nfs-mnj-hot-01/tmp/masomatics/block_diag/'\n",
    "result_dir = '/mnt/nfs-mnj-hot-01/tmp/masomatics/block_diag/result'\n",
    "\n",
    "\n",
    "\n",
    "os.listdir(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea5954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_filenameso3dat_sphere_MLPpt_nameSeqAELSTSQ_LinearNet',\n",
       " 'data_filenameso3dat_sphere_MLPpt_nameSeqAELSTSQ_so3Net']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datamode='MLP'\n",
    "#query = 'dat_sphere_Linearpt'\n",
    "query = f\"\"\"dat_sphere_{datamode}pt\"\"\"\n",
    "targlist = filter_list(query, os.listdir(exp_result))\n",
    "targlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2016e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapshot_model_iter_1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 313/313 [00:01<00:00, 227.72it/s]\n",
      "/usr/local/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapshot_model_iter_1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 189.75it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {} \n",
    "inferred_Ms = {} \n",
    "tp = 3\n",
    "n_cond = 5\n",
    "\n",
    "\n",
    "for targdir in targlist:\n",
    "    \n",
    "    Mlist = [] \n",
    "    targdir_path = os.path.join(exp_result, targdir)\n",
    "    config = load_config(targdir_path)\n",
    "    \n",
    "#     dataconfig = config['train_data']\n",
    "#     dataconfig['args']['T'] = tp + n_cond\n",
    "\n",
    "    dataconfig = config['train_data']\n",
    "    original_datfile = dataconfig['args']['data_filename']\n",
    "    trans_datfile = original_datfile.split('.')[0] + '_shared_trans.pt'  \n",
    "    dataconfig['args']['data_filename'] = trans_datfile   \n",
    "    dataconfig['args']['T'] = tp + n_cond\n",
    "    \n",
    "    data = yu.load_component(dataconfig)\n",
    "    train_loader = DataLoader(data, \n",
    "                              batch_size=config['batchsize'],\n",
    "                              shuffle=True,\n",
    "                              num_workers=config['num_workers'])\n",
    "    model = yu.load_component(config['model'])\n",
    "    iterlist = iter_list(targdir_path)\n",
    "    \n",
    "    if len(iterlist) == 0:\n",
    "        print(f\"\"\"There is no model trained for {targdir_path}\"\"\")\n",
    "    else:\n",
    "        maxiter = np.max(iter_list(targdir_path))\n",
    "        load_model(model, targdir_path, maxiter)\n",
    "        model = model.eval().to(device)\n",
    "\n",
    "        # Initialize lazy modules\n",
    "        images = iter(train_loader).next()\n",
    "        images = images.to(device)\n",
    "        model(images[:, :2])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = [] \n",
    "            for images in tqdm(train_loader):\n",
    "                images = images.to(device)\n",
    "                images_cond = images[:, :n_cond]\n",
    "                images_target = images[:, n_cond:n_cond+tp]\n",
    "                M = model.get_M(images_cond) #n a a\n",
    "                H = model.encode(images_cond[:, -1:])[:, 0] # n s a\n",
    "\n",
    "                xs = []\n",
    "                for r in range(tp):\n",
    "                    H = H @ M \n",
    "                    x_next_t = model.decode(H[:, None])\n",
    "                    xs.append(x_next_t)\n",
    "\n",
    "                x_next = torch.cat(xs, axis=1)\n",
    "                r2_losses = [] \n",
    "                for k in range(tp):\n",
    "                    r2_loss_t = r2_score(images_target[:,k].to('cpu').numpy(), x_next[:,k].to('cpu').numpy()) \n",
    "                    r2_losses.append(r2_loss_t)\n",
    "                scores.append(torch.tensor(r2_losses))\n",
    "                Mlist.append(M)\n",
    "        \n",
    "                \n",
    "        Mlist = torch.cat(Mlist)       \n",
    "        scores = torch.stack(scores)\n",
    "        av_score = torch.mean(scores, axis=0)\n",
    "        av_var = torch.var(scores, axis=0)\n",
    "        results[targdir] = [av_score, av_var]\n",
    "        inferred_Ms[targdir] = Mlist \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63802740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_filenameso3dat_sphere_MLPpt_nameSeqAELSTSQ_LinearNet': [tensor([0.6080, 0.3566, 0.2411], dtype=torch.float64),\n",
       "  tensor([0.0007, 0.0019, 0.0020], dtype=torch.float64)],\n",
       " 'data_filenameso3dat_sphere_MLPpt_nameSeqAELSTSQ_so3Net': [tensor([0.6687, 0.4831, 0.3950], dtype=torch.float64),\n",
       "  tensor([0.0007, 0.0017, 0.0023], dtype=torch.float64)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80d62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
