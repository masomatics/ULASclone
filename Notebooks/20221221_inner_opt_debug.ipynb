{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383e6977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.dynamics_models' from '../models/dynamics_models.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "from models import dynamics_models as dm\n",
    "from importlib import reload\n",
    "import torch.autograd as autograd\n",
    "from models import misc_mnet as mnet\n",
    "\n",
    "reload(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5f11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "inner_args = {'normalize': 0, 'batchsize': 32, 'num_loops': 20, 'detach': 1, 'beta': 0, 'temperature': 1, 'mode': 'exact', 'lr': 0.01, 'dim_a': 16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4fc135",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H0H1.pkl', 'rb') as fp:\n",
    "    problem = pickle.load(fp)\n",
    "H0 = problem['H0']\n",
    "H1 = problem['H1']\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "trueM = np.random.normal(size = (H0.shape[0], 16, 16))\n",
    "trueH1 = H0 @ trueM\n",
    "inner_args['num_loops']=200\n",
    "inner_args['lr']=0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc36c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30da6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_obj = dm.LinearTensorDynamicsLSTSQ_inner(inner_args=inner_args, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5ff4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "inner_loss 3827.4614566244873\n",
      "inner_loss 3732.5447568478608\n",
      "inner_loss 3637.8722627095913\n",
      "inner_loss 3543.4503328203828\n",
      "inner_loss 3449.2856173397713\n",
      "inner_loss 3355.3850748648606\n",
      "inner_loss 3261.755989324226\n",
      "inner_loss 3168.4059884303883\n",
      "inner_loss 3075.3430841285904\n",
      "inner_loss 2982.5756802561705\n",
      "inner_loss 2890.1126159477885\n",
      "inner_loss 2797.963188311904\n",
      "inner_loss 2706.137194065424\n",
      "inner_loss 2614.644964567518\n",
      "inner_loss 2523.497409040959\n",
      "inner_loss 2432.7060664575556\n",
      "inner_loss 2342.2831532210066\n",
      "inner_loss 2252.241624246099\n",
      "inner_loss 2162.5952422839805\n",
      "inner_loss 2073.358644398747\n",
      "inner_loss 1984.547436134521\n",
      "inner_loss 1896.1782768374917\n",
      "inner_loss 1808.2689933522659\n",
      "inner_loss 1720.8386996352783\n",
      "inner_loss 1633.9079385112427\n",
      "inner_loss 1547.4988437483992\n",
      "inner_loss 1461.6353319933012\n",
      "inner_loss 1376.343316191814\n",
      "inner_loss 1291.650971284287\n",
      "inner_loss 1207.5890415982997\n",
      "inner_loss 1124.1912113707058\n",
      "inner_loss 1041.4945538588997\n",
      "inner_loss 959.5400763220724\n",
      "inner_loss 878.3734273328639\n",
      "inner_loss 798.0457456811364\n",
      "inner_loss 718.6148111687178\n",
      "inner_loss 640.1465462395395\n",
      "inner_loss 562.7170726229801\n",
      "inner_loss 486.41566283675434\n",
      "inner_loss 411.34912867644704\n",
      "inner_loss 337.6488702632735\n",
      "inner_loss 265.483147433298\n",
      "inner_loss 195.08149628300583\n",
      "inner_loss 126.79477364547587\n",
      "inner_loss 61.32461075959183\n",
      "inner_loss 8.563550588064116\n",
      "inner_loss 69.18485093611125\n",
      "inner_loss 18.299924307560303\n",
      "inner_loss 76.32078388151174\n",
      "inner_loss 24.081721298086517\n",
      "inner_loss 82.7228305282366\n",
      "inner_loss 28.70040803482446\n",
      "inner_loss 90.02979387325804\n",
      "inner_loss 33.08960276946891\n",
      "inner_loss 95.6735537469749\n",
      "inner_loss 35.087325075750485\n",
      "inner_loss 97.67429802147358\n",
      "inner_loss 35.615407254447824\n",
      "inner_loss 98.20699446014386\n",
      "inner_loss 35.776227803037656\n",
      "inner_loss 98.39468482755143\n",
      "inner_loss 35.853539999459485\n",
      "inner_loss 98.50128304535303\n",
      "inner_loss 35.90950163105257\n",
      "inner_loss 98.58611330554584\n",
      "inner_loss 35.95912149270648\n",
      "inner_loss 98.66408299366118\n",
      "inner_loss 36.006464274664374\n",
      "inner_loss 98.73935472015248\n",
      "inner_loss 36.05270087240595\n",
      "inner_loss 98.8130775619303\n",
      "inner_loss 36.09810395130807\n",
      "inner_loss 98.88543180310761\n",
      "inner_loss 36.14260573707928\n",
      "inner_loss 98.95620498678885\n",
      "inner_loss 36.18598332399533\n",
      "inner_loss 99.02500473333241\n",
      "inner_loss 36.22795897162273\n",
      "inner_loss 99.09136128150166\n",
      "inner_loss 36.268253580455756\n",
      "inner_loss 99.15481380664515\n",
      "inner_loss 36.30657309952289\n",
      "inner_loss 99.21492968418868\n",
      "inner_loss 36.34267426887716\n",
      "inner_loss 99.27134988340487\n",
      "inner_loss 36.376370640258436\n",
      "inner_loss 99.32382014628729\n",
      "inner_loss 36.40753853956339\n",
      "inner_loss 99.37217816719738\n",
      "inner_loss 36.43611212850088\n",
      "inner_loss 99.41636925137851\n",
      "inner_loss 36.46209866317382\n",
      "inner_loss 99.45643583326216\n",
      "inner_loss 36.48555115632022\n",
      "inner_loss 99.49249494821235\n",
      "inner_loss 36.50657654356867\n",
      "inner_loss 99.52472885475959\n",
      "inner_loss 36.52530888349592\n",
      "inner_loss 99.55337992411347\n",
      "inner_loss 36.54190309083437\n",
      "inner_loss 99.57871601171026\n",
      "inner_loss 36.55652012662862\n",
      "inner_loss 99.6010044554751\n",
      "inner_loss 36.56936243407101\n",
      "inner_loss 99.6205313712036\n",
      "inner_loss 36.5805877829158\n",
      "inner_loss 99.63759358751444\n",
      "inner_loss 36.59036870882746\n",
      "inner_loss 99.65244316308666\n",
      "inner_loss 36.59886614325609\n",
      "inner_loss 99.6653448686795\n",
      "inner_loss 36.60624374471218\n",
      "inner_loss 99.67651389845756\n",
      "inner_loss 36.612629777456384\n",
      "inner_loss 99.68616372225942\n",
      "inner_loss 36.618148324895294\n",
      "inner_loss 99.69450242515991\n",
      "inner_loss 36.62290473725294\n",
      "inner_loss 99.70169653755518\n",
      "inner_loss 36.62700073179357\n",
      "inner_loss 99.70789222119669\n",
      "inner_loss 36.63053273935181\n",
      "inner_loss 99.71321135210358\n",
      "inner_loss 36.633578384149025\n",
      "inner_loss 99.71779205507156\n",
      "inner_loss 36.636185924305316\n",
      "inner_loss 99.72172423775035\n",
      "inner_loss 36.638426272515005\n",
      "inner_loss 99.72511020874047\n",
      "inner_loss 36.64034866074864\n",
      "inner_loss 99.72801662475557\n",
      "inner_loss 36.64199906542797\n",
      "inner_loss 99.73050828900506\n",
      "inner_loss 36.643415212904365\n",
      "inner_loss 99.73265080949407\n",
      "inner_loss 36.64463918602545\n",
      "inner_loss 99.73448191269738\n",
      "inner_loss 36.6456825721805\n",
      "inner_loss 99.73605625381572\n",
      "inner_loss 36.646580225661026\n",
      "inner_loss 99.73740743260926\n",
      "inner_loss 36.64734805852993\n",
      "inner_loss 99.73856480659411\n",
      "inner_loss 36.6480130535043\n",
      "inner_loss 99.73955534576505\n",
      "inner_loss 36.64857627128517\n",
      "inner_loss 99.74041240598015\n",
      "inner_loss 36.649069918452575\n",
      "inner_loss 99.7411337392461\n",
      "inner_loss 36.649490053178916\n",
      "inner_loss 99.74176600191028\n",
      "inner_loss 36.64984993188424\n",
      "inner_loss 99.74230674346572\n",
      "inner_loss 36.65014924596891\n",
      "inner_loss 99.74277331429604\n",
      "inner_loss 36.650406850439424\n",
      "inner_loss 99.74317271464871\n",
      "inner_loss 36.650640200142\n",
      "inner_loss 99.7435075829005\n",
      "inner_loss 36.65083194601939\n",
      "inner_loss 99.74380382683817\n",
      "inner_loss 36.651005590503736\n",
      "inner_loss 99.74403637059766\n",
      "inner_loss 36.65116113498721\n",
      "inner_loss 99.7442444704973\n",
      "inner_loss 36.65129515819305\n",
      "inner_loss 99.7444205660905\n",
      "inner_loss 36.6514074428232\n",
      "inner_loss 99.74457453026743\n",
      "inner_loss 36.65149578077511\n",
      "inner_loss 99.74471074034956\n",
      "inner_loss 36.65158048556467\n",
      "inner_loss 99.74481926714647\n",
      "inner_loss 36.65165657821309\n",
      "inner_loss 99.74491147800799\n",
      "inner_loss 36.65172266412991\n",
      "inner_loss 99.74499572467452\n",
      "inner_loss 36.6517699655578\n",
      "inner_loss 99.74506514762815\n",
      "inner_loss 36.65181399928642\n",
      "inner_loss 99.74513328715078\n",
      "inner_loss 36.651849183613884\n",
      "inner_loss 99.74518967894777\n",
      "inner_loss 36.651880204371665\n",
      "inner_loss 99.74523918252616\n",
      "inner_loss 36.65190767706078\n",
      "inner_loss 99.74527809404981\n",
      "inner_loss 36.65192821132511\n",
      "inner_loss 99.74531425470626\n",
      "inner_loss 36.65194799233323\n",
      "inner_loss 99.74534349785712\n",
      "inner_loss 36.65196627974665\n",
      "inner_loss 99.74537327371813\n",
      "inner_loss 36.651977075778525\n",
      "inner_loss 99.7453921695141\n",
      "inner_loss 36.65199157742688\n",
      "inner_loss 99.74541055617887\n",
      "inner_loss 36.65199813494866\n",
      "inner_loss 99.74543607745902\n",
      "inner_loss 36.65200766757086\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "Mstar = dy_obj.compute_M(H0, trueH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807cf6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.6243, -0.6118, -0.5282, -1.0730],\n",
       "         [-0.1724, -0.8779,  0.0422,  0.5828],\n",
       "         [-0.6872, -0.8452, -0.6712, -0.0127],\n",
       "         [ 0.1202,  0.6172,  0.3002, -0.3522]], grad_fn=<SliceBackward>),\n",
       " array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862],\n",
       "        [-0.17242821, -0.87785842,  0.04221375,  0.58281521],\n",
       "        [-0.6871727 , -0.84520564, -0.67124613, -0.0126646 ],\n",
       "        [ 0.12015895,  0.61720311,  0.30017032, -0.35224985]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mstar[0][:4, :4], trueM[0][:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8331b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2975.7557, dtype=torch.float64, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.sum((H0@Mstar[:H0.shape[0]]- trueH1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d453e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b6f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d23e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ef6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562da00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b9f992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/523019437.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Meta_M = Variable(torch.tensor(torch.eye(inner_args['dim_a'])), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "Meta_M = Variable(torch.tensor(torch.eye(inner_args['dim_a'])), requires_grad=True)\n",
    "trueM_two = np.random.normal(size = (16, 16))\n",
    "\n",
    "H0p = torch.tensor(np.random.normal(size=(H0.shape))).float()\n",
    "H1hat = H0p @ trueM_two\n",
    "\n",
    "inner_args['lr']=0.01\n",
    "inner_args['num_loops']=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1550d1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_loss 1440.6858096199555\n",
      "inner_loss 1435.1323727087522\n",
      "inner_loss 1435.4294134078739\n",
      "inner_loss 1440.70853552326\n",
      "inner_loss 1450.0891540032812\n",
      "inner_loss 1462.7352938213508\n",
      "inner_loss 1477.8927941927084\n",
      "inner_loss 1494.907948424617\n",
      "inner_loss 1513.2317452140246\n",
      "inner_loss 1532.4145671460635\n",
      "inner_loss 1552.095711255076\n",
      "inner_loss 1571.9908055448618\n",
      "inner_loss 1591.8791305376897\n",
      "inner_loss 1611.5919792407444\n",
      "inner_loss 1631.0024934877033\n",
      "inner_loss 1650.0170887994427\n",
      "inner_loss 1668.5684189456865\n",
      "inner_loss 1686.609683032316\n",
      "inner_loss 1704.110049850928\n",
      "inner_loss 1721.051058728957\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "with torch.enable_grad():\n",
    "\n",
    "    for j in range(inner_args['num_loops']):\n",
    "        loss = torch.sqrt(torch.sum((H0p@Meta_M - H1hat)**2))\n",
    "        print(f\"\"\"inner_loss {loss.item()}\"\"\")\n",
    "        grads = autograd.grad(loss, Meta_M,\n",
    "                              create_graph=(not inner_args['detach']),\n",
    "                              only_inputs=True, allow_unused=True)\n",
    "        # parameter update\n",
    "        # for param, grad in zip(self.mobject.parameters(), grads):\n",
    "        #\n",
    "        #     new_param = param - self.inner_args['lr'] * grad\n",
    "        #     #new_param = param - 0.001 * grad\n",
    "        #\n",
    "        #     param.data.copy_(new_param)\n",
    "        Meta_M = Meta_M - inner_args['lr'] * grads[0][0]\n",
    "print(\"-\"*10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1c1efbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03ee9d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39/559703700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMeta_M\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "Meta_M.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8b20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f48dfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalize': 0,\n",
       " 'batchsize': 32,\n",
       " 'num_loops': 20,\n",
       " 'detach': 1,\n",
       " 'beta': 0,\n",
       " 'temperature': 1,\n",
       " 'mode': 'exact',\n",
       " 'lr': 0.01,\n",
       " 'dim_a': 16}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43183671",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H0H1.pkl', 'rb') as fp:\n",
    "    problem = pickle.load(fp)\n",
    "H0 = problem['H0']\n",
    "H1 = problem['H1']\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "trueM = np.random.normal(size = (H0.shape[0], 16, 16))\n",
    "trueH1 = H0 @ trueM\n",
    "inner_args['num_loops']=200\n",
    "inner_args['lr']=0.01  \n",
    "\n",
    "\n",
    "\n",
    "Meta_M_obj = mnet.Meta_Mnet(**inner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26bf954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_loss 1561.4898083037324\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39/1178007781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnew_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minner_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation."
     ]
    }
   ],
   "source": [
    "with torch.enable_grad():\n",
    "\n",
    "    for j in range(inner_args['num_loops']):\n",
    "        loss =  Meta_M_obj(H0, H1hat)\n",
    "        print(f\"\"\"inner_loss {loss.item()}\"\"\")\n",
    "        grads = autograd.grad(loss, Meta_M_obj.parameters(),\n",
    "                              create_graph=(not inner_args['detach']),\n",
    "                              only_inputs=True, allow_unused=True)\n",
    "        for param, grad in zip(Meta_M_obj.parameters(), grads):\n",
    "        \n",
    "            new_param = param - inner_args['lr'] * grad\n",
    "        \n",
    "            param.data.copy_(new_param)\n",
    "        \n",
    "print(\"-\"*10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abaa9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_loss 3889.787199871139\n",
      "inner_loss 2792.8971687002277\n",
      "inner_loss 2684.051761452392\n",
      "inner_loss 2678.1238944291563\n",
      "inner_loss 2677.6739869674684\n",
      "inner_loss 2677.629693329063\n",
      "inner_loss 2677.623908730731\n",
      "inner_loss 2677.6228541944038\n",
      "inner_loss 2677.6226021516727\n",
      "inner_loss 2677.62253230466\n",
      "inner_loss 2677.622512086098\n",
      "inner_loss 2677.6225056735566\n",
      "inner_loss 2677.622503809284\n",
      "inner_loss 2677.622503347045\n",
      "inner_loss 2677.6225033246465\n",
      "inner_loss 2677.622503158332\n",
      "inner_loss 2677.622502989294\n",
      "inner_loss 2677.622503079189\n",
      "inner_loss 2677.6225031763\n",
      "inner_loss 2677.622503056528\n",
      "inner_loss 2677.622503180004\n",
      "inner_loss 2677.6225030693067\n",
      "inner_loss 2677.6225029835796\n",
      "inner_loss 2677.6225031918543\n",
      "inner_loss 2677.62250289674\n",
      "inner_loss 2677.622502863409\n",
      "inner_loss 2677.6225031131503\n",
      "inner_loss 2677.622503137011\n",
      "inner_loss 2677.6225032623415\n",
      "inner_loss 2677.6225031148124\n",
      "inner_loss 2677.6225033618935\n",
      "inner_loss 2677.6225031510357\n",
      "inner_loss 2677.6225030674627\n",
      "inner_loss 2677.6225033608007\n",
      "inner_loss 2677.6225031847416\n",
      "inner_loss 2677.6225030839955\n",
      "inner_loss 2677.6225030053147\n",
      "inner_loss 2677.6225031124172\n",
      "inner_loss 2677.622503013815\n",
      "inner_loss 2677.622503209729\n",
      "inner_loss 2677.6225029884167\n",
      "inner_loss 2677.622503099455\n",
      "inner_loss 2677.6225031230615\n",
      "inner_loss 2677.6225030638025\n",
      "inner_loss 2677.622503127189\n",
      "inner_loss 2677.6225030679266\n",
      "inner_loss 2677.6225032906254\n",
      "inner_loss 2677.622503058118\n",
      "inner_loss 2677.6225031013314\n",
      "inner_loss 2677.6225032631896\n",
      "inner_loss 2677.622503075069\n",
      "inner_loss 2677.6225031327463\n",
      "inner_loss 2677.6225030281103\n",
      "inner_loss 2677.622503084946\n",
      "inner_loss 2677.622503117078\n",
      "inner_loss 2677.6225029944276\n",
      "inner_loss 2677.622502989089\n",
      "inner_loss 2677.622503221833\n",
      "inner_loss 2677.62250332912\n",
      "inner_loss 2677.622503171125\n",
      "inner_loss 2677.622503243394\n",
      "inner_loss 2677.622503073863\n",
      "inner_loss 2677.622503115696\n",
      "inner_loss 2677.622503064583\n",
      "inner_loss 2677.622503186154\n",
      "inner_loss 2677.6225030276896\n",
      "inner_loss 2677.622503072443\n",
      "inner_loss 2677.6225031354393\n",
      "inner_loss 2677.622503207937\n",
      "inner_loss 2677.6225031165004\n",
      "inner_loss 2677.6225031798617\n",
      "inner_loss 2677.6225030283863\n",
      "inner_loss 2677.622503138032\n",
      "inner_loss 2677.6225031489957\n",
      "inner_loss 2677.622503167111\n",
      "inner_loss 2677.6225031193876\n",
      "inner_loss 2677.6225031909303\n",
      "inner_loss 2677.6225029762154\n",
      "inner_loss 2677.622503262268\n",
      "inner_loss 2677.6225030195874\n",
      "inner_loss 2677.622503193028\n",
      "inner_loss 2677.6225030973173\n",
      "inner_loss 2677.6225031914837\n",
      "inner_loss 2677.6225029954026\n",
      "inner_loss 2677.622503120255\n",
      "inner_loss 2677.622503023917\n",
      "inner_loss 2677.6225030456712\n",
      "inner_loss 2677.6225030789974\n",
      "inner_loss 2677.622503171995\n",
      "inner_loss 2677.6225031122544\n",
      "inner_loss 2677.6225031067315\n",
      "inner_loss 2677.6225032665807\n",
      "inner_loss 2677.6225030662454\n",
      "inner_loss 2677.6225032133684\n",
      "inner_loss 2677.6225029172624\n",
      "inner_loss 2677.622503094244\n",
      "inner_loss 2677.622503108176\n",
      "inner_loss 2677.6225030359547\n",
      "inner_loss 2677.6225031735853\n",
      "inner_loss 2677.6225033256164\n",
      "inner_loss 2677.622503061098\n",
      "inner_loss 2677.622503219341\n",
      "inner_loss 2677.6225031640956\n",
      "inner_loss 2677.6225031077875\n",
      "inner_loss 2677.622502965736\n",
      "inner_loss 2677.6225031841373\n",
      "inner_loss 2677.6225030438995\n",
      "inner_loss 2677.6225031780973\n",
      "inner_loss 2677.622503175562\n",
      "inner_loss 2677.6225030875066\n",
      "inner_loss 2677.6225031055296\n",
      "inner_loss 2677.622503114267\n",
      "inner_loss 2677.622503126064\n",
      "inner_loss 2677.6225032520742\n",
      "inner_loss 2677.622503276552\n",
      "inner_loss 2677.622503020212\n",
      "inner_loss 2677.6225031732683\n",
      "inner_loss 2677.6225031475433\n",
      "inner_loss 2677.6225031330328\n",
      "inner_loss 2677.622503171707\n",
      "inner_loss 2677.6225030145742\n",
      "inner_loss 2677.6225031161675\n",
      "inner_loss 2677.622503074208\n",
      "inner_loss 2677.6225031637377\n",
      "inner_loss 2677.6225030688433\n",
      "inner_loss 2677.622503120128\n",
      "inner_loss 2677.622503080489\n",
      "inner_loss 2677.6225032329694\n",
      "inner_loss 2677.6225031075405\n",
      "inner_loss 2677.622503199414\n",
      "inner_loss 2677.622503165312\n",
      "inner_loss 2677.622503058873\n",
      "inner_loss 2677.6225031049294\n",
      "inner_loss 2677.6225030637793\n",
      "inner_loss 2677.622503147756\n",
      "inner_loss 2677.622503015114\n",
      "inner_loss 2677.622503142499\n",
      "inner_loss 2677.6225032830566\n",
      "inner_loss 2677.6225031313516\n",
      "inner_loss 2677.6225032202933\n",
      "inner_loss 2677.6225030348364\n",
      "inner_loss 2677.6225031745003\n",
      "inner_loss 2677.6225031080075\n",
      "inner_loss 2677.6225031277945\n",
      "inner_loss 2677.6225030277787\n",
      "inner_loss 2677.6225031617864\n",
      "inner_loss 2677.622503097572\n",
      "inner_loss 2677.622503134859\n",
      "inner_loss 2677.6225031650156\n",
      "inner_loss 2677.622503140098\n",
      "inner_loss 2677.6225032206285\n",
      "inner_loss 2677.6225032252523\n",
      "inner_loss 2677.6225031866393\n",
      "inner_loss 2677.622503125087\n",
      "inner_loss 2677.6225030999253\n",
      "inner_loss 2677.6225030889536\n",
      "inner_loss 2677.6225031434856\n",
      "inner_loss 2677.6225030973624\n",
      "inner_loss 2677.6225031441495\n",
      "inner_loss 2677.6225032447996\n",
      "inner_loss 2677.62250307853\n",
      "inner_loss 2677.6225030784262\n",
      "inner_loss 2677.622503185515\n",
      "inner_loss 2677.6225031767335\n",
      "inner_loss 2677.6225031309127\n",
      "inner_loss 2677.6225030868127\n",
      "inner_loss 2677.622503257187\n",
      "inner_loss 2677.6225030935075\n",
      "inner_loss 2677.6225030253654\n",
      "inner_loss 2677.622503130721\n",
      "inner_loss 2677.6225031021695\n",
      "inner_loss 2677.6225031959843\n",
      "inner_loss 2677.6225031820295\n",
      "inner_loss 2677.622503213009\n",
      "inner_loss 2677.6225031057193\n",
      "inner_loss 2677.622503177687\n",
      "inner_loss 2677.622503256931\n",
      "inner_loss 2677.622503170602\n",
      "inner_loss 2677.6225031429376\n",
      "inner_loss 2677.6225031472295\n",
      "inner_loss 2677.6225031141626\n",
      "inner_loss 2677.6225032534276\n",
      "inner_loss 2677.6225031406157\n",
      "inner_loss 2677.6225031935783\n",
      "inner_loss 2677.6225031288463\n",
      "inner_loss 2677.6225032391194\n",
      "inner_loss 2677.622503056716\n",
      "inner_loss 2677.622503152009\n",
      "inner_loss 2677.6225030819232\n",
      "inner_loss 2677.622503055664\n",
      "inner_loss 2677.6225031028225\n",
      "inner_loss 2677.622503162111\n",
      "inner_loss 2677.62250320775\n",
      "inner_loss 2677.6225032426178\n",
      "inner_loss 2677.6225030765154\n",
      "inner_loss 2677.622503091361\n",
      "inner_loss 2677.622503232376\n",
      "inner_loss 2677.622503268959\n",
      "inner_loss 2677.6225031362237\n",
      "inner_loss 2677.622503170293\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "verbose=True\n",
    "Mobj = mnet.Meta_Mnet(dim_a=16, batchsize=32, mode='exact')\n",
    "#Mobj.Ms.data = copy.deepcopy(problem['M'])\n",
    "#Mobj.Ms.data = torch.eye(16).float()\n",
    "Mobj.Ms.data = torch.tensor(np.random.normal(size=(16, 16))).float()\n",
    "with torch.enable_grad():\n",
    "\n",
    "    for j in range(inner_args['num_loops']):\n",
    "        loss = Mobj(H0, trueH1)\n",
    "        grads = autograd.grad(loss, Mobj.parameters(),\n",
    "                              create_graph=(not inner_args['detach']),\n",
    "                              only_inputs=True, allow_unused=True)\n",
    "        # parameter update\n",
    "        for param, grad in zip(Mobj.parameters(), grads):\n",
    "\n",
    "            #new_param = param - self.inner_args['lr'] * grad\n",
    "            new_param = param - 0.1 * grad\n",
    "\n",
    "            param.data.copy_(new_param)\n",
    "        if verbose: print(f\"\"\"inner_loss {loss.item()}\"\"\")\n",
    "    if verbose: print(\"-\"*10)\n",
    "Mstar = Mobj.Ms.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532a2df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_loss 2801.8849043935033\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39/1112315034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             F.sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    111\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation."
     ]
    }
   ],
   "source": [
    "reload(mnet)\n",
    "Mobj = mnet.Meta_Mnet(dim_a=16, batchsize=32, mode='exact')\n",
    "optimizer = torch.optim.SGD(Mobj.parameters(), inner_args['lr'])\n",
    "\n",
    "for k in range(inner_args['num_loops']):\n",
    "    loss = Mobj(H0, trueH1)\n",
    "    print(f\"\"\"inner_loss {loss.item()}\"\"\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a37a4f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1109, -2.4911,  1.7026,  ..., -0.4438, -1.3853, -1.1067],\n",
       "         [-1.7529,  2.8129, -2.4634,  ...,  2.4947, -1.7529, -1.5583],\n",
       "         [-2.0048, -2.1789, -2.0868,  ..., -2.2929,  1.9419, -1.1916],\n",
       "         ...,\n",
       "         [ 1.0895, -1.4626,  2.6042,  ...,  1.7241,  0.4403, -0.4087],\n",
       "         [ 1.8314, -2.6205, -2.5710,  ...,  0.9191, -2.5827, -1.9482],\n",
       "         [-1.5201,  1.8679, -2.0569,  ...,  2.2132,  2.3692,  2.5352]],\n",
       "\n",
       "        [[-1.8675, -2.2506,  1.3423,  ..., -0.1323, -0.9872, -0.6825],\n",
       "         [-2.0292,  2.6892, -2.0201,  ...,  1.9581, -1.6889, -1.2036],\n",
       "         [-2.0917, -2.0253, -2.2447,  ..., -2.4576,  2.4662, -1.4204],\n",
       "         ...,\n",
       "         [ 1.5217, -1.2507,  2.4432,  ...,  1.4848,  0.5541, -0.4833],\n",
       "         [ 1.9434, -2.5379, -2.5964,  ...,  0.9308, -2.5965, -1.9227],\n",
       "         [-1.7474,  1.9326, -2.1185,  ...,  2.1439,  2.3942,  2.2156]],\n",
       "\n",
       "        [[-1.8111, -2.4227,  1.1315,  ...,  0.6633, -0.9333, -0.6034],\n",
       "         [-2.2678,  2.4463, -2.2146,  ...,  2.0949, -1.7862, -1.5246],\n",
       "         [-2.1868, -1.8025, -1.9311,  ..., -2.7089,  2.0914, -1.4752],\n",
       "         ...,\n",
       "         [ 1.7997, -1.9206,  2.5009,  ...,  1.2916,  0.5451, -0.7242],\n",
       "         [ 2.3548, -2.4102, -2.4025,  ...,  1.2885, -2.4253, -2.2300],\n",
       "         [-1.9601,  1.8976, -2.3585,  ...,  2.4106,  2.3659,  2.6501]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.0595, -2.5724,  1.5949,  ...,  0.5111, -0.6516, -0.8535],\n",
       "         [-1.8806,  2.2935, -2.5194,  ...,  2.4674, -1.9014, -1.1899],\n",
       "         [-1.8351, -1.9257, -2.3365,  ..., -2.4012,  1.2290, -1.5367],\n",
       "         ...,\n",
       "         [ 1.5066, -1.8926,  2.9295,  ...,  1.6565,  0.4074, -0.5318],\n",
       "         [ 2.3562, -2.5914, -2.5343,  ...,  1.4988, -2.7349, -2.3455],\n",
       "         [-2.0439,  1.9173, -1.9898,  ...,  2.6305,  2.2397,  2.5762]],\n",
       "\n",
       "        [[-1.5273, -2.4635,  1.2293,  ...,  0.0884, -0.8125, -0.4783],\n",
       "         [-1.9888,  2.2649, -2.0266,  ...,  1.9927, -1.6599, -1.1453],\n",
       "         [-1.6699, -2.0118, -2.1320,  ..., -2.2852,  2.0820, -1.6776],\n",
       "         ...,\n",
       "         [ 1.3777, -1.4064,  2.4452,  ...,  1.4249,  0.2947, -0.5435],\n",
       "         [ 2.5412, -2.4087, -2.4878,  ...,  0.8666, -2.4903, -2.2973],\n",
       "         [-1.7219,  2.0067, -1.8655,  ...,  2.2644,  2.1549,  2.4808]],\n",
       "\n",
       "        [[-1.8735, -2.4459,  1.3187,  ...,  0.3313, -1.0590, -0.5795],\n",
       "         [-2.2442,  2.6338, -2.3300,  ...,  1.9468, -1.9927, -1.1508],\n",
       "         [-1.9894, -2.0286, -2.1114,  ..., -2.4916,  2.2827, -1.4256],\n",
       "         ...,\n",
       "         [ 1.5529, -1.6225,  2.6275,  ...,  1.9574,  0.7852, -0.5576],\n",
       "         [ 2.5675, -2.3841, -2.3330,  ...,  1.3269, -2.5059, -1.9928],\n",
       "         [-1.6186,  1.9880, -2.1668,  ...,  2.3334,  2.0312,  2.5557]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7211fcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0258, -2.1756,  1.6909,  ..., -0.4623, -1.2928, -1.0002],\n",
       "         [-1.8270,  2.9528, -2.3075,  ...,  2.7862, -1.7894, -1.6834],\n",
       "         [-2.0330, -2.0211, -2.3528,  ..., -2.1145,  1.8305, -0.8984],\n",
       "         ...,\n",
       "         [ 1.3440, -1.3396,  2.2562,  ...,  1.5883,  0.3422, -0.3967],\n",
       "         [ 1.8721, -2.6913, -2.2773,  ...,  1.0807, -2.2359, -1.8012],\n",
       "         [-1.6778,  1.9449, -2.0263,  ...,  2.0222,  2.2672,  2.5387]],\n",
       "\n",
       "        [[-2.0626, -2.1800,  1.4792,  ...,  0.2510, -1.0677, -0.7235],\n",
       "         [-2.0879,  2.7920, -2.2749,  ...,  2.1215, -1.8976, -1.5059],\n",
       "         [-1.9622, -1.9252, -2.0342,  ..., -2.6619,  2.3244, -1.5062],\n",
       "         ...,\n",
       "         [ 1.3953, -1.3354,  2.5230,  ...,  1.8409,  0.5737, -0.6423],\n",
       "         [ 2.1262, -2.6986, -2.3735,  ...,  1.1961, -2.3959, -1.9148],\n",
       "         [-1.5842,  2.2761, -2.1835,  ...,  2.0623,  2.0234,  2.4233]],\n",
       "\n",
       "        [[-1.4912, -2.2418,  1.3095,  ...,  0.4815, -1.1305, -0.3138],\n",
       "         [-2.0136,  2.3749, -2.2645,  ...,  2.2513, -1.4073, -1.5080],\n",
       "         [-1.9946, -1.7243, -2.2668,  ..., -2.3651,  2.2706, -1.7530],\n",
       "         ...,\n",
       "         [ 1.5050, -1.7536,  2.2178,  ...,  1.1627,  0.7203, -0.8490],\n",
       "         [ 2.2316, -2.5459, -2.4301,  ...,  1.0563, -2.4153, -2.5724],\n",
       "         [-1.8568,  2.1320, -2.2399,  ...,  2.4431,  2.5021,  2.7849]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.9970, -2.5777,  1.5455,  ...,  0.3283, -0.7585, -0.6716],\n",
       "         [-2.0471,  2.6399, -2.3548,  ...,  2.2813, -1.7771, -1.0001],\n",
       "         [-1.9781, -1.9788, -2.5052,  ..., -2.5570,  1.3597, -1.4635],\n",
       "         ...,\n",
       "         [ 1.4252, -1.6891,  2.6470,  ...,  1.6858, -0.0959, -0.6329],\n",
       "         [ 2.3606, -2.4184, -2.1277,  ...,  1.1120, -2.6751, -2.2971],\n",
       "         [-2.1999,  1.7695, -2.0597,  ...,  2.4269,  2.0686,  2.5685]],\n",
       "\n",
       "        [[-1.6172, -2.3102,  1.0922,  ...,  0.1086, -0.4740, -0.8143],\n",
       "         [-2.1539,  2.3033, -1.9068,  ...,  2.0897, -1.8662, -1.4010],\n",
       "         [-1.5866, -1.9301, -1.5830,  ..., -2.5774,  2.1090, -1.6792],\n",
       "         ...,\n",
       "         [ 1.6385, -1.5988,  2.2274,  ...,  1.4265,  0.6215, -0.7065],\n",
       "         [ 2.2516, -2.2019, -2.3099,  ...,  1.4187, -2.4873, -2.1135],\n",
       "         [-1.7837,  1.7304, -2.0701,  ...,  2.0706,  1.9050,  2.4024]],\n",
       "\n",
       "        [[-1.7729, -2.3350,  1.3060,  ...,  0.3524, -1.0484, -0.6314],\n",
       "         [-2.2031,  2.8614, -2.1775,  ...,  1.9380, -1.7903, -1.0919],\n",
       "         [-2.1730, -1.8642, -2.0412,  ..., -2.4312,  2.4710, -1.6037],\n",
       "         ...,\n",
       "         [ 1.2629, -1.6386,  2.5602,  ...,  1.7224,  1.0088, -0.6993],\n",
       "         [ 2.3781, -2.3504, -2.3610,  ...,  1.0500, -2.6644, -2.0104],\n",
       "         [-1.5282,  1.9555, -2.1008,  ...,  2.2178,  2.2489,  2.5459]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458ee60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
